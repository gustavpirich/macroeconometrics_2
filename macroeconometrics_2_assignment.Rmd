---
title: '**Advanced Macroeconometrics -- Assignment 2**'
author:
  - "Gustav Pirich"
  - "Sannah Tijani"
  - "Martin Prinz"
date: "May 10, 2023"
output: 
  pdf_document:
    toc: true
    includes:
header-includes: 
   - \usepackage{tcolorbox}
   - \usepackage{bm}
papersize: a4
geometry: margin = 2cm
urlcolor: Mahogany
---

```{r, setup, include = FALSE}
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 80), tidy = TRUE)
set.seed(123)
library(ggplot2)
library(bayesAB)
```

\vspace{2em}


\newpage

# Exercise 1

## Simulation from draws of Normal Distribution
```{r, out.width="70%", fig.align="center"}
random <- rnorm(100, mean = 5, sd = 9)

#Mean with varying 1, ..., n draws
mean_est <- (cumsum(random) / (1:100))

#par(mar = c(4, 4, 2, 2)) # bottom, left, top, right
plot(1:100, mean_est, type = "l", xlab = "n", ylab = "Estimate", col = "blue", lwd = 2,
     main = "Convergence of sample mean to population mean from N(5, 9)")
abline(h = 5, col = "red")
legend("bottomright", legend = "Actual mean", col = "red", lty = 1)
```
\
As n increases, the estimate converges to the true mean of the simulated distribution. The estimated mean converges to the true sample mean. This is exactly what we would expect based on the law of large numbers.

## Simulation from draws of Cauchy distirbution
```{r, out.width="70%", fig.align="center"}
set.seed(123)

# Simulate 10000 draws from a standard Cauchy distribution
cauchy <- (rnorm(n = 10000, mean = 0, sd = 1) / rnorm(n = 10000, mean = 0, sd = 1))
mean_est <- cumsum(cauchy) / (1:10000)

# Change the margins
plot(1:10000, mean_est, type = "l", xlab = "n", ylab = "Estimate", col = "blue", lwd = 2,
     main = "Simulated from standard Cauchy distribution")
abline(h = 0, col = "red")
legend("bottomright", legend = "Mean of the two standard normal distributions", col = "red", lty = 1)
```
\
The quotient of two standard normal distribution follows a Cauchy distribution. The moments of a Cauchy distribution do not exist (it's a pathological distribution). The law of large numbers does not apply. That is exactly why we do not observe any convergence in the estimates of the mean. 

\newpage

# Exercise 2 

## 2.1 Deriviing the Posterior distribution
The class of conjugate priors is the Beta distribution. We now derive the posterior distribution

The likelihood follows a binomial distribution. As in the slides, we settled on a Beta distribution to specify the prior. Since the binomial distribution is conjugate to the beta distribution, the posterior follows a beta distribution. ADDING DERIVATIONS

## 2.2 Several Point Estimates

## The assumption of identical and independent errors 
A source of prior information could be the baseline prevelance of the disease in Austria in the last week. More specific data could also be used like, how many people in the office already had COVID. Further potential useful informaiton is how many times they were infected and how high their vaccination rates are. 

## 2. 4 The assumption of identical and independent errors 
The assumption of independent and identically distributed data seems implausible. If one individual has the disease on a given day, the probability that the other tests are positive is higher. So we would expect the outbreak to be correlated in time dimension. The knowledge of the correlation could be used to impose structure on the dynamics of the infection. To improve the conceptual understanding of the dynamics of the infection rates, one could impose a dependence structure. First one could model the spread of the disease as a timer series, where one could include an autoregressive term to account for correlation in time. Second one could model the disease prevelance as a network. One could ac count for the fact that people spend differentially more time together than others, increasing the likelihoof of transmission. Thinking about disease prevalnce in terms of networks seems to use the conceptually most realistic approximation of the disease prevalence 




\newpage

# Exercise 3

## 3.1 Simulating data
```{r}
set.seed(123) 
# The function returns a list with y and X
simulate_linear_regression <- function(n, k, alpha, beta, sigma) {
  # Draw k independent variables from a log normal distribution
  X <- matrix(rlnorm(n * k), nrow = n, ncol = k)
  # Draw the error term from a normal distribution with mean zero and standard deviation sigma
  e <- rnorm(n, mean = 0, sd = sigma)
  # Compute y as alpha + X*beta + e
  y <- alpha + X %*% beta + e
  # Return a list with y and X
  return(list(y = y, X = X))
}
```
\
We wrote a function that simulates data from a linear regression. For the k-independent variables, we have chosen a lognormal distribution. (rlnorm)

```{r}
set.seed(123) 
data <- simulate_linear_regression(n = 100, k = 1, alpha = 2, beta = 2, sigma = 1)
y <- data$y # Extract y from the list
X <- data$X # Extract X from the list

# Plot x and y in a scatterplot and add a least-squares regression line
plot(X, y, main = "Simulated data", xlab = "x", ylab = "y")
abline(lm(y ~ X), col = "red") # Fit a linear model and add the line
```
\
Because we generated the X values with lognormal distribution, we can see a clustering of values for lower values of X. 
```{r}
set.seed(123) 
# Repeat this 1000 times and store beta_LS every time
beta_LS <- numeric(1000) # Initialize a vector to store beta_LS
for (i in 1:1000) {
  data <- simulate_linear_regression(n = 100, k = 1, alpha = 1, beta = 2, sigma = 1)
  y <- data$y
  X <- data$X
  fit <- lm(y ~ X) # Fit a linear model
  beta_LS[i] <- coef(fit)[2] # Extract the second coefficient (beta_LS)
}

# Create a histogram of the LS estimates
hist(beta_LS, main = "Histogram of Beta Estimates ", xlab = "beta_LS", breaks = 100)
```
\
We create a histogram of the LS estimators. We see that the histogram is centered at 2, the true value of beta with which the data was generated. The distribution seems to follow a normal distribution. The beta estimates are also fairly precise. Even the "outer" estimates at around 1.8-2.2 are not too too far off from the true coefficient.

## 3.2 Latent values 
Given that we know that $\sigma^{2} = 1$, the only latent values are the $beta$ coefficients. 

## 3.3 Income on Happiness 
In class we discussed the relation between income on happiness. Let's assume we want to test this hypothesis in a more aggregated macro context- We could investigate this relationship in developing countries by running the following regression. 

\[
  Happiness_{i,t}= \alpha_i + \alpha_t + \beta ln(Nighttimelights_{i,t}) + \epsilon_{i,t}
\]

Happiness might be operationalized via surveys in specific regions, countries or grids. Let us assume that the survey item takes on a continuous range from 0-10, where 10 is happy, 0 unhappy. Nighttime lights are being used as a proxy for income. If a region grows more prosperous, we should observe an increase in nighttime lights. We suspect that the effect of an increase in economic growth on happiness is positive in low-income countries. Thus our prior could take on the following form. If we expect a positive effect, then we could establish the prior to follow an N(0.3,0.25), N(0.5,0.5), or N(1.75, 0.6). A large mean and small variance for a prior can be justifiable if other empirical studies have already established a positive link in developing countries. 

```{r}
set.seed(123) 
x <- seq(-5, 5, length=1000)
mu_1 <- 0.2
sigma_1 <- 0.25
mu_2 <- 0.5
sigma_2 <- 0.5
mu_3 <- 1.75
sigma_3 <- 0.6

plot(x, dnorm(x, mu_1, sigma_1), type="l", col="red", lwd=2,
     ylim=c(0,2), xlab="x", ylab="Density")
lines(x, dnorm(x, mu_2, sigma_2), type="l", col="blue", lwd=2)
lines(x, dnorm(x, mu_3, sigma_3), type="l", col="green", lwd=2)
legend("topright", legend=c("N(0.2,0.25)", "N(0.5,0.5)", "N(1.75,0.6)"),
       col=c("red", "blue", "green"), lty=1)
```


## 3.4 Simulating posterior density 
\
We specify a prior. We decide on N(1.5, 2). We now compute and plot the posterior density specified as: 
```{r}
set.seed(95)

#specifying the prior N(15, 1^2)
sigma_prior <- 1
mu_prior <- 15

simul_50 <- simulate_linear_regression(n = 50, k = 1, alpha  = 1 , beta = 3, sigma = 1)
simul_100 <- simulate_linear_regression(n = 100, k = 1, alpha  = 1, beta = 3, sigma = 1)
simul_200 <- simulate_linear_regression(n = 200, k = 1, alpha  = 1, beta = 3, sigma = 1)

X_50 <- simul_50$X
Y_50 <- simul_50$y

X_100 <- simul_100$X
Y_100 <- simul_100$y

X_200 <- simul_200$X
Y_200 <- simul_200$y

sigma_50 <- as.numeric((sigma_prior^(-1) + ((t(X_50) %*% X_50)))^(-1))
mu_50 <- as.numeric(sigma_50*(sigma_prior^(-1)*mu_prior + (t(X_50) %*% Y_50)))

sigma_100 <- as.numeric((sigma_prior^(-1) + ((t(X_100)%*%X_100)))^(-1))
mu_100 <- as.numeric(sigma_100*(sigma_prior^(-1)*mu_prior + (t(X_100)%*%Y_100)))

sigma_200 <- as.numeric((sigma_prior^(-1) + ((t(X_200)%*%X_200)))^(-1))
mu_200 <- as.numeric(sigma_200*(sigma_prior^(-1)*mu_prior + (t(X_200)%*%Y_200)))

x <- seq(3.1, 3.45, by = 0.0001)

plot(x, dnorm(x, mu_50, sigma_50), type="l", col="red", lwd=2, xlab="x", ylim = c(0,1000), ylab="Density", main = "Posterior Distributions for Prior N(5, 2)")
lines(x, dnorm(x, mu_100, sigma_100), type="l", col="blue", lwd=2)
lines(x, dnorm(x, mu_200, sigma_200), type="l", col="green", lwd=2)
legend("topright", legend=c("n = 50", "n = 100", "n = 200"),
       col=c("red", "blue", "green"), lty=1)

```

Our true coefficient is $\beta = 3$. We select a fairly incorrect prior (N(15,1)) to observe the impact of increasing n. We observe that as we increase the sample size of the simulation, the posterior distribution becomes significantly more precise and more accurate to the true values of $\beta$. 
\newpage

# Exercise 4 

## 4.1 
We use the fact that $\sum_{i=1}^n (y_i - \beta)^2 = n(\beta - \Bar{y}) + (\sum_{i=1}^n (y_i-\Bar{y})^2)$ (Variance Decomposition)

\begin{align*}
    p(\mu|y) &\propto p(y|1,\mu)p(\mu) \\
    &\propto \exp(-\frac{n(\mu-\Bar{y})^2}{2} - \frac{\sum_{i=1}^n (y_i-\Bar{y})^2}{2})\exp(-\frac{(\mu-\mu_0)^2}{2\sigma_0^2}) \\
    & \propto \exp(-\frac{n(\mu^2 - 2\mu\Bar{y} + \Bar{y}^2)}{2} - \frac{\mu^2 - 2\mu\mu_0 +\mu_0^2}{2\sigma_0^2})\\
    &= \exp(-\frac{\sigma_0^2(\mu^2 - 2\mu\Bar{y} + \Bar{y}^2)}{2\sigma_0^2/n} - \frac{(1/n)(\mu^2 - 2\mu\mu_0 +\mu_0^2)}{(2\sigma_0^2)/n})\\
    &= \exp(-\frac{1}{2\sigma_0^2/n}(\sigma_0^2(\mu^2 - 2\mu\Bar{y} + \Bar{y}^2) + \frac{1}{n}(\mu^2 - 2\mu\mu_0 +\mu_0^2))) \\
    &= \exp(-\frac{1}{2\sigma_0^2/n}(\mu^2(\sigma_0^2 + 1/n) - 2\mu(\Bar{y}\sigma_0^2 + \mu_0/n) + \Bar{y}^2\sigma_0^2 + \mu_0^2/n))) \\
    &\propto \exp(-\frac{1}{2\sigma_0^2/n}(\mu^2(\sigma_0^2 + 1/n) - 2\mu(\Bar{y}\sigma_0^2 + \mu_0/n))) \\
    &= \exp(-\frac{1}{2\sigma_0^2/n}(\mu^2(\sigma_0^2 + 1/n) - 2\mu(\Bar{y}\sigma_0^2 + \mu_0/n)\frac{(\sigma_0^2 + 1/n)}{(\sigma_0^2 + 1/n)})) \\
    &= \exp(-\frac{(\sigma_0^2 + 1/n)}{2\sigma_0^2/n}(\mu^2 - 2\mu(\Bar{y}\sigma_0^2 + \mu_0/n)\frac{1}{(\sigma_0^2 + 1/n)})) \\
    &\propto \exp(-\frac{(\sigma_0^2 + 1/n)}{2\sigma_0^2/n}(\mu^2 - 2\mu\frac{(\Bar{y}\sigma_0^2 + \mu_0/n)}{(\sigma_0^2 + 1/n)} + (\frac{(\Bar{y}\sigma_0^2 + \mu_0/n)}{(\sigma_0^2 + 1/n)})^2)) \\
    &= \exp(-\frac{1}{2\sigma^\ast} (\mu - \mu^\ast)
\end{align*}
where $\mu^\ast = \frac{(\Bar{y}\sigma_0^2 + \mu_0/n)}{(\sigma_0^2 + 1/n)})^2)$ and $\sigma^\ast = (n+\frac{1}{\sigma_0^2})^{-1}$

\\

```{r, out.width = '75%', echo = FALSE, fig.align = 'center'}
# Create two vectors of data
data1 <- rnorm(1000, 12, 1)
data2 <- rnorm(1000, 11, 1)

df <- data.frame(value = c(data1, data2),
                 group = c(rep("data1", length(data1)), rep("data2", length(data2))))

ggplot(df, aes(x=value, fill=group)) +
  geom_histogram(alpha=0.5, position="identity", bins=30) +
  scale_fill_manual(values=c("red", "blue")) +
  labs(title="Histograms of Two Normally Distributed Priors N(12,1) and N(11,1)", x="Value", y="Frequency")


```

# 4.2 
```{r, out.width = '75%',echo=FALSE, center, fig.align = 'center'}
x_lower_g <- 0
x_upper_g <- 1

ggplot(data.frame(x = c(x_lower_g , x_upper_g)), aes(x = x)) + 
  xlim(c(x_lower_g , x_upper_g)) + 
  stat_function(fun = dgamma, args = list(rate = 0.01, shape = 0.5), aes(colour = "0.5 & 0.01")) + 
  stat_function(fun = dgamma, args = list(rate = 1, shape = 0.5), aes(colour = "0.5 & 1")) + 
  stat_function(fun = dgamma, args = list(rate = 100, shape = 0.5), aes(colour = "0.5 & 100")) + 
  scale_color_manual("Shape & Rate \n Parameters", values = c("black", "blue", "red")) +
  labs(y = "Density", 
       title = "Gamma Distribution Plots") + 
  theme(plot.title = element_text(hjust = 0.5), 
        axis.title.x = element_text(colour="black", size = 12),
        axis.title.y = element_text(colour="black", size = 12),
        legend.title = element_text(size = 10),
        legend.position = "right")
```
